#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
"""

import time
import threading
from datetime import datetime
from load_test_manager import LoadTestManager


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = []
        
    def run_benchmark(self, bot_counts: list, duration_per_test: int = 120):
        """ë‹¤ì–‘í•œ ë´‡ ìˆ˜ë¡œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print("ğŸ† ì±„íŒ…ì„œë²„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        print("="*60)
        
        for bot_count in bot_counts:
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {bot_count}ê°œ ë´‡ ({duration_per_test}ì´ˆ)")
            print("-" * 40)
            
            result = self.test_bot_count(bot_count, duration_per_test)
            self.results.append(result)
            
            self.print_test_result(result)
            
            # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ì ê¹ ëŒ€ê¸°
            if bot_count != bot_counts[-1]:
                print("â³ ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘... (10ì´ˆ ëŒ€ê¸°)")
                time.sleep(10)
        
        # ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        self.print_benchmark_summary()
    
    def test_bot_count(self, bot_count: int, duration: int) -> dict:
        """íŠ¹ì • ë´‡ ìˆ˜ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            manager = LoadTestManager("localhost", 9200)
            manager.create_bots(bot_count)
            manager.start_test(duration)
            
            # ì—°ê²° ì™„ë£Œ ëŒ€ê¸°
            time.sleep(min(30, bot_count // 5))
            
            # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
            start_time = time.time()
            latencies = []
            message_counts = []
            
            # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë™ì•ˆ ì„±ëŠ¥ ì¸¡ì •
            measurement_start = time.time()
            while time.time() - measurement_start < duration - 30:
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ì¸¡ì •
                
                stats = manager.get_current_stats()
                if stats['latency']:
                    latencies.append(stats['latency']['avg'])
                message_counts.append(stats['messages_sent'])
            
            # ìµœì¢… í†µê³„
            final_stats = manager.get_current_stats()
            manager.stop_test()
            
            return {
                'bot_count': bot_count,
                'duration': duration,
                'connected_bots': final_stats['connected_bots'],
                'success_rate': (final_stats['connected_bots'] / bot_count) * 100,
                'avg_latency': sum(latencies) / len(latencies) if latencies else 0,
                'min_latency': min(latencies) if latencies else 0,
                'max_latency': max(latencies) if latencies else 0,
                'total_messages': final_stats['messages_sent'],
                'messages_per_second': final_stats['messages_sent'] / duration if duration > 0 else 0,
                'error_count': final_stats['error_count'],
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            return {
                'bot_count': bot_count,
                'error': str(e),
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def print_test_result(self, result: dict):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        if 'error' in result:
            print(f"âŒ {result['bot_count']}ê°œ ë´‡ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
            return
        
        print(f"ğŸ“Š {result['bot_count']}ê°œ ë´‡ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"   ğŸ”— ì—°ê²° ì„±ê³µë¥ : {result['success_rate']:.1f}%")
        print(f"   âš¡ í‰ê·  ì‘ë‹µì‹œê°„: {result['avg_latency']:.1f}ms")
        print(f"   ğŸ“ˆ ì‘ë‹µì‹œê°„ ë²”ìœ„: {result['min_latency']:.1f}ms ~ {result['max_latency']:.1f}ms")
        print(f"   ğŸ’¬ ì´ˆë‹¹ ë©”ì‹œì§€: {result['messages_per_second']:.1f} msg/s")
        print(f"   âŒ ì˜¤ë¥˜ ìˆ˜: {result['error_count']}")
        
        # ì„±ëŠ¥ ë“±ê¸‰
        avg_latency = result['avg_latency']
        if avg_latency < 20:
            grade = "ğŸ† ìµœê³ ê¸‰"
        elif avg_latency < 40:
            grade = "â­ ìš°ìˆ˜"
        elif avg_latency < 80:
            grade = "âœ… ì–‘í˜¸"
        else:
            grade = "âš ï¸ ê°œì„ í•„ìš”"
        
        print(f"   ğŸ¯ ì„±ëŠ¥ ë“±ê¸‰: {grade}")
    
    def print_benchmark_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ê²°ê³¼"""
        print("\n" + "="*60)
        print("ğŸ† ìµœì¢… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*60)
        
        successful_results = [r for r in self.results if 'error' not in r]
        
        if not successful_results:
            print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë´‡ ìˆ˜ë³„ ì„±ëŠ¥ í‘œ
        print(f"{'ë´‡ ìˆ˜':>6} | {'ì„±ê³µë¥ ':>7} | {'ì‘ë‹µì‹œê°„':>8} | {'ì²˜ë¦¬ëŸ‰':>10} | {'ë“±ê¸‰':>8}")
        print("-" * 55)
        
        for result in successful_results:
            bot_count = result['bot_count']
            success_rate = result['success_rate']
            latency = result['avg_latency']
            throughput = result['messages_per_second']
            
            # ë“±ê¸‰ ê²°ì •
            if latency < 20:
                grade = "ìµœê³ ê¸‰"
            elif latency < 40:
                grade = "ìš°ìˆ˜"
            elif latency < 80:
                grade = "ì–‘í˜¸"
            else:
                grade = "ê°œì„ í•„ìš”"
            
            print(f"{bot_count:>6} | {success_rate:>6.1f}% | {latency:>6.1f}ms | {throughput:>8.1f}/s | {grade:>8}")
        
        # ê¶Œì¥ ë™ì ‘ì ìˆ˜
        best_latency = min(r['avg_latency'] for r in successful_results)
        best_result = next(r for r in successful_results if r['avg_latency'] == best_latency)
        
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
        print(f"   ğŸ† ìµœê³  ì„±ëŠ¥: {best_result['bot_count']}ê°œ ë´‡ì—ì„œ {best_latency:.1f}ms")
        
        # ì‹¤ìš©ì  ê¶Œì¥ ìˆ˜ì¹˜
        practical_results = [r for r in successful_results if r['avg_latency'] < 50 and r['success_rate'] > 90]
        if practical_results:
            max_practical = max(r['bot_count'] for r in practical_results)
            print(f"   ğŸ’¡ ê¶Œì¥ ë™ì ‘ì: ìµœëŒ€ {max_practical}ëª… (50ms ì´í•˜, 90% ì´ìƒ ì„±ê³µë¥ )")
        
        # í™•ì¥ì„± ì˜ˆì¸¡
        if len(successful_results) >= 2:
            # ì„ í˜• ì¦ê°€ ê°€ì •ìœ¼ë¡œ ì˜ˆì¸¡
            last_result = successful_results[-1]
            if last_result['avg_latency'] < 100:
                predicted_500 = (last_result['avg_latency'] / last_result['bot_count']) * 500
                predicted_1000 = (last_result['avg_latency'] / last_result['bot_count']) * 1000
                
                print(f"   ğŸ”® í™•ì¥ì„± ì˜ˆì¸¡:")
                print(f"      500ëª…: ì•½ {predicted_500:.0f}ms")
                print(f"      1000ëª…: ì•½ {predicted_1000:.0f}ms")
        
        print("="*60)


def main():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸ† ì±„íŒ…ì„œë²„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    print()
    
    # í…ŒìŠ¤íŠ¸í•  ë´‡ ìˆ˜ ì„¤ì •
    bot_counts = [10, 25, 50, 75, 100]
    
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ê³„íš: {bot_counts} ê°œ ë´‡")
    print("â±ï¸  ê° í…ŒìŠ¤íŠ¸: 2ë¶„ê°„ ì§„í–‰")
    print("ğŸ“Š ì¸¡ì • í•­ëª©: ì‘ë‹µì‹œê°„, ì„±ê³µë¥ , ì²˜ë¦¬ëŸ‰")
    print()
    
    confirm = input("ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if confirm.lower() != 'y':
        print("ë²¤ì¹˜ë§ˆí¬ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = PerformanceBenchmark()
    try:
        benchmark.run_benchmark(bot_counts, duration_per_test=120)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ë²¤ì¹˜ë§ˆí¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ë²¤ì¹˜ë§ˆí¬ ì˜¤ë¥˜: {e}")
    
    print("\nğŸ‘‹ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
